{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee41a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 01:58:15.679699: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                    filename  LE_left  LE_top  \\\n",
      "0           0  001/0/2022-09-26 08:07:22.792702-37704.jpg      131     410   \n",
      "1           1  001/0/2022-09-26 08:07:23.101953-37752.jpg      163     479   \n",
      "2           2  001/0/2022-09-26 08:07:22.818864-37708.jpg      130     410   \n",
      "3           3  001/0/2022-09-26 08:07:23.225207-37772.jpg      183     523   \n",
      "4           4  001/0/2022-09-26 08:07:23.126469-37756.jpg      167     481   \n",
      "\n",
      "   LE_right  LE_bottom  RE_left  RE_top  RE_right  RE_bottom   lx   ly   rx  \\\n",
      "0       375        525      633     429       856        545  252  470  766   \n",
      "1       379        564      621     479       821        584  260  531  740   \n",
      "2       373        525      632     431       854        547  255  471  763   \n",
      "3       380        593      624     526       807        610  261  572  723   \n",
      "4       379        566      622     480       822        583  262  540  733   \n",
      "\n",
      "    ry  relative_lx  relative_ly  relative_rx  relative_ry  \n",
      "0  483          121           60          133           54  \n",
      "1  537           97           52          119           58  \n",
      "2  486          125           61          131           55  \n",
      "3  578           78           49           99           52  \n",
      "4  546           95           59          111           66  \n",
      "Columns in DataFrame: Index(['Unnamed: 0', 'filename', 'LE_left', 'LE_top', 'LE_right', 'LE_bottom',\n",
      "       'RE_left', 'RE_top', 'RE_right', 'RE_bottom', 'lx', 'ly', 'rx', 'ry',\n",
      "       'relative_lx', 'relative_ly', 'relative_rx', 'relative_ry'],\n",
      "      dtype='object')\n",
      "Training set size: 123\n",
      "Validation set size: 27\n",
      "Test set size: 27\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "csv_file_path = '/Users/shubhibhadauriya/Desktop/Diss Project/001 2/001_0.csv'\n",
    "image_base_path = '/Users/shubhibhadauriya/Desktop/Diss Project/0'  \n",
    "\n",
    "\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "\n",
    "print(data.head())\n",
    "print(\"Columns in DataFrame:\", data.columns)\n",
    "\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "image_width, image_height = 640, 480\n",
    "data['lx'] = data['lx'] / image_width\n",
    "data['ly'] = data['ly'] / image_height\n",
    "data['rx'] = data['rx'] / image_width\n",
    "data['ry'] = data['ry'] / image_height\n",
    "\n",
    "\n",
    "features = ['lx', 'ly', 'rx', 'ry']\n",
    "\n",
    "\n",
    "def find_image_file(csv_filename, image_dir):\n",
    "   \n",
    "    parts = csv_filename.split('/')\n",
    "    timestamp_id = parts[-1].split('-')[0]  \n",
    "\n",
    "    \n",
    "    corrected_timestamp_id = timestamp_id.replace(':', '/')\n",
    "    \n",
    "    \n",
    "    for root, dirs, files in os.walk(image_dir):\n",
    "        for file in files:\n",
    "            if corrected_timestamp_id in file:\n",
    "                return os.path.join(root, file)\n",
    "    return None  \n",
    "\n",
    "\n",
    "def load_images(filenames, base_path):\n",
    "    images = []\n",
    "    for filename in filenames:\n",
    "        img_path = find_image_file(filename, base_path)\n",
    "        if img_path and os.path.exists(img_path):\n",
    "            img = load_img(img_path, target_size=(image_height, image_width), color_mode='rgb')\n",
    "            img_array = img_to_array(img)\n",
    "            images.append(img_array)\n",
    "        else:\n",
    "            print(f\"File not found for CSV entry: {filename}\")\n",
    "            images.append(np.zeros((image_height, image_width, 3)))  \n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "filenames = data['filename'].values\n",
    "X_images = load_images(filenames, image_base_path)\n",
    "\n",
    "\n",
    "y = data[features].values\n",
    "\n",
    "\n",
    "if X_images.shape[0] != y.shape[0]:\n",
    "    raise ValueError(f\"Inconsistent number of images ({X_images.shape[0]}) and labels ({y.shape[0]}).\")\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_images, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d9e5fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initial_learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m X_test_flat \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Build the dense model with the correct input shape\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m dense_model \u001b[38;5;241m=\u001b[39m build_dense_model(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[43minitial_learning_rate\u001b[49m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Define callbacks for optimization\u001b[39;00m\n\u001b[1;32m     26\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initial_learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def build_dense_model(learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_shape=(X_train_flat.shape[1],), activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(4, activation='linear')  # Assuming you have 4 output features: lx, ly, rx, ry\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "\n",
    "dense_model = build_dense_model(learning_rate=initial_learning_rate)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "\n",
    "dense_history = dense_model.fit(X_train_flat, y_train, \n",
    "                                validation_data=(X_val_flat, y_val), \n",
    "                                epochs=epochs, \n",
    "                                batch_size=batch_size,\n",
    "                                callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate on test data\n",
    "##dense_test_loss, dense_test_mae = dense_model.evaluate(X_test_flat, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09f344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_keras_env)",
   "language": "python",
   "name": "tf_keras_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
